---
title: "Author Attribution"
author: "Pedro Ivo Rivas"
date: "8/19/2019"
output: 
  md_document:
    variant: gfm
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# AUTHOR ATTRIBUTION  

## Introduction  

Data was received in a format where a train folder contained folders for 50 authors and each author folder contained 50 documents for each author. It also contained a test folder with a similar structure with the same 50 authors and 50 other documents written by each author.

```{r, include = FALSE}
rm(list =ls())

library(tm) 
library(magrittr)
library(slam)
library(proxy)
library(e1071)
library(dplyr)
library(caret)
library(naivebayes)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
```

## Method  

First we read the author folder names and looped through the contents to get the document information and content. This was done for both train and test sets. With this data we made a corpus document, which is a text mining document. We then mapped five content transformers to make everything lowercase and remove numbers, punctuation, excess whitespace and stop words. Next, we created a document term matrix that is a data frame like structure that comprises rows for each document and columns for all terms that appear across documents. Once this was created, sparse terms were removed on the training and test document term matrices using 91% and 96% thresholds respectively (these thresholds are somewhat arbitrary and different values could be used). Finally, we removed terms that occur in the test data but not in the train data.

```{r, include = FALSE}
authors_train = Sys.glob('./ReutersC50/C50train/*')
file_list_train = NULL
labels_train = NULL

for(author in authors_train) {
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_train = append(file_list_train, files_to_add)
  author_name = substring(author, first=23)
  labels_train = append(labels_train, rep(author_name, length(files_to_add)))
}

train = lapply(file_list_train, readerPlain) 
mynames = file_list_train %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist
# Rename the articles
names(train) = mynames

#labels_train

authors_test = Sys.glob('./ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL

for(author in authors_test) {
  files_to_add_test = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add_test)
  author_name_test = substring(author, first=22)
  labels_test = append(labels_test, rep(author_name_test, length(files_to_add_test)))
}
test = lapply(file_list_test, readerPlain) 
mynames = file_list_test %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist
names(test) = mynames
```
```{r, include=FALSE}
## TRAIN ##
documents_raw = Corpus(VectorSource(train))
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))

## TEST ###
file_list_test = Sys.glob('./ReutersC50/C50test/*')
names(test) = mynames
documents_raw_test = Corpus(VectorSource(test))
my_documents_test = documents_raw_test
my_documents_test = tm_map(my_documents_test, content_transformer(tolower)) # make everything lowercase
my_documents_test = tm_map(my_documents_test, content_transformer(removeNumbers)) # remove numbers
my_documents_test = tm_map(my_documents_test, content_transformer(removePunctuation)) # remove punctuation
my_documents_test = tm_map(my_documents_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents_test = tm_map(my_documents_test, content_transformer(removeWords), stopwords("en"))

```
```{r, include=FALSE}
DTM_train = DocumentTermMatrix(my_documents)
DTM_test = DocumentTermMatrix(my_documents_test)

DTM_train <- removeSparseTerms(DTM_train, 0.91)
DTM_train_freq <- as.data.frame(as.matrix(DTM_train))
DTM_train_freq <- as.data.frame(as.matrix(DTM_train_freq))
DTM_test <- removeSparseTerms(DTM_test, 0.96)
DTM_test <- as.data.frame(as.matrix(DTM_test))

```
```{r, echo=FALSE}
DTM_test_norm <- DTM_test[ ,(names(DTM_test) %in% names(DTM_train_freq))]
train = as.matrix(DTM_train_freq)
test =  as.matrix(DTM_test_norm)
train_Y =  as.factor(labels_train)
test_Y = as.factor(labels_test)
train_df = cbind(as.data.frame(train), as.data.frame(train_Y))

classifier <-  naive_bayes(train, train_Y, laplace = 1)
pred <- predict(classifier, test, type = 'class')
pred_tab <- table("Predictions"= pred,  "Actual" = labels_test)
NB_r <- sum(diag(pred_tab))/sum(pred_tab)
```

## Models  

First we ran Naive-Bayes to predict authors. The data was changed to matrices to run the Naive-Bayes library and had predictive accuracy of approximately `r NB_r`%. This uses the naivebayes library in R. 

```{r, echo=FALSE}
library(randomForest)
colnames(train_df) <- paste(colnames(train_df), "_c", sep = "")
test_rf <- test
colnames(test_rf) <- paste(colnames(test), "_c", sep = "")
model_rf <- randomForest(train_Y_c ~ . ,data = train_df, ntree=1250, mtry = 20) # Non cv

pred_rf <- predict(model_rf, test_rf, type = 'class')
pred_tab_rf <- table("Predictions"= pred_rf,  "Actual" = labels_test)
rf_r <- sum(diag(pred_tab_rf))/sum(pred_tab_rf)
```

Secondly we ran a Random Forest model with 1,250 trees and an m of 20. To run this in R, we had to change some terms in the train and test data sets so they did not conflict with special terms in R. The Random Forest model had predictive accuracy of approximately `r rf_r`%. This uses the randomForest library in R. 
